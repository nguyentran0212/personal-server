# Local AI Stack

This stack is developed for both local development and utilisation. 

Noted that OLLAMA running inside Docker CANNOT use GPU on Macbook. 